>From a7a4b76deba838684de2a544c24799911a29d715 Mon Sep 17 00:00:00 2001
From: Pete Delaney <piet.delaney@imgtec.com>
Date: Tue, 26 Nov 2013 17:24:17 -0800
Subject: [PATCH] A few modification to Wolfgang and Steven Qiu's SMP cache
 flush changes.

Added calls to disable preemption on paths not necessarily using
the r4k_on_other_cpu() and r4k_on_each_cpu() code paths; merged
with Steven Qiu's disabling of preemption but waiting for confirmation
of Hit_Writeback_Invalidate broadcasting to coherent caches.

Using effective addresses protected_blast_other_cpu_dcache_range_ip()
which don't set any of the unused bits in the Index cache op address
fields.

Android browser runs fine and is much faster (2x ... 3x).
Below are measurements of doing a Google for 'mips' and
switching between 'Web', 'Images', and two of the Images.

             4.3    4.4 Fixed (This Commit)   4.4 (Slow Igwang's latest)
             -----  -----------------------   --------------------------
Web->Images:  10s   7s                        27s
Images->Web   4s    6s                        10s
Web->Images   9s    8s                        21s
Images->1st   3s    4s                        11s
Images->2nd   3s    3s                         5s

flushtest over the past two weekends has been rock solid.
Steven Qiu's  agreed with the disabling of preemption being
necessary, and similar tests have shown the system to be solid;
merged in his changes and added a few questions/notes.

Enabled KERNEL DEBUG for now, see little downside on leaving it
on by default.

Change-Id: I653a9a5d5341b10689acf983f0f7f502cb2e946b
Signed-off-by: Pete Delaney <piet.delaney@imgtec.com>
---
 arch/mips/configs/npm801_nand_defconfig |   5 +-
 arch/mips/include/asm/rjzcache.h        |   9 +
 arch/mips/mm/c-jz.c                     | 327 ++++++++++++++++++++++++--------
 3 files changed, 258 insertions(+), 83 deletions(-)

diff --git a/arch/mips/configs/npm801_nand_defconfig b/arch/mips/configs/npm801_nand_defconfig
index cfe2505..2cb8f0c 100755
--- a/arch/mips/configs/npm801_nand_defconfig
+++ b/arch/mips/configs/npm801_nand_defconfig
@@ -2053,7 +2053,7 @@ CONFIG_SCHED_DEBUG=y
 # CONFIG_SLUB_DEBUG_ON is not set
 # CONFIG_SLUB_STATS is not set
 # CONFIG_DEBUG_KMEMLEAK is not set
-# CONFIG_DEBUG_PREEMPT is not set
+CONFIG_DEBUG_PREEMPT=y
 # CONFIG_DEBUG_RT_MUTEXES is not set
 # CONFIG_RT_MUTEX_TESTER is not set
 CONFIG_DEBUG_SPINLOCK=y
@@ -2068,7 +2068,8 @@ CONFIG_STACKTRACE=y
 # CONFIG_DEBUG_STACK_USAGE is not set
 # CONFIG_DEBUG_KOBJECT is not set
 # CONFIG_DEBUG_HIGHMEM is not set
-# CONFIG_DEBUG_INFO is not set
+CONFIG_DEBUG_INFO=y
+# CONFIG_DEBUG_INFO_REDUCED is not set
 # CONFIG_DEBUG_VM is not set
 # CONFIG_DEBUG_WRITECOUNT is not set
 # CONFIG_DEBUG_MEMORY_INIT is not set
diff --git a/arch/mips/include/asm/rjzcache.h b/arch/mips/include/asm/rjzcache.h
index 1ab8b8a..09d5b27 100644
--- a/arch/mips/include/asm/rjzcache.h
+++ b/arch/mips/include/asm/rjzcache.h
@@ -12,6 +12,8 @@
 #ifndef _ASM_R4KCACHE_H
 #define _ASM_R4KCACHE_H
 
+#include <linux/hardirq.h>		/* for preemptible() decl */
+
 #include <asm/asm.h>
 #include <asm/cacheops.h>
 #include <asm/cpu-features.h>
@@ -559,6 +561,10 @@ static inline void blast_dcache_jz(void)
 	unsigned long start = (unsigned long)reserved_for_alloccache;
 	unsigned long end = start + current_cpu_data.dcache.waysize * current_cpu_data.dcache.ways;
 	unsigned long addr = start;
+
+       if ( preemptible() )
+		panic("%s: preemptible!\n", __func__);
+
        do {
 		i_pref(JZ_ALLOC_PREF, addr, 0);i_lw(addr);i_cache(Hit_Invalidate_D,addr,0);addr += 32;
 		i_pref(JZ_ALLOC_PREF, addr, 0);i_lw(addr);i_cache(Hit_Invalidate_D,addr,0);addr += 32;
@@ -647,6 +653,9 @@ static inline void blast_dcache32(void)
 	                       current_cpu_data.dcache.waybit;
 	unsigned long ws, addr;
 
+	if ( preemptible() )
+		panic("%s: preemptible!\n", __func__);
+
 	for (ws = 0; ws < ws_end; ws += ws_inc)
 		for (addr = start; addr < end; addr += 0x400)
 			cache32_unroll32(addr|ws,Index_Writeback_Inv_D);
diff --git a/arch/mips/mm/c-jz.c b/arch/mips/mm/c-jz.c
index 9fbdeaa..a792e14 100644
--- a/arch/mips/mm/c-jz.c
+++ b/arch/mips/mm/c-jz.c
@@ -66,6 +66,10 @@ static inline void r4k_on_other_cpu(void (*func) (void *info), void *info)
 #endif
 }
 
+/*
+ * Data cache index ops are likely safe, but,
+ * Instruction cache index ops are not safe.
+ */
 #if defined(CONFIG_MIPS_CMP)
 #define cpu_has_safe_index_cacheops 0
 #else
@@ -154,8 +158,28 @@ static void (* r4k_blast_dcache)(void);
 static void __cpuinit r4k_blast_dcache_setup(void)
 {
 	unsigned long dc_lsize = cpu_dcache_line_size();
-	//r4k_blast_dcache_jz = blast_dcache_jz;
+	/*
+	 * NOTE:
+	 *	blast_dcache_jz():
+	 *	    Special Ingenic version that tried to flush the data cache with
+	 *	    Address cache op Hit_Invalidate_D used to avoid flushing
+	 *	    additional ways.
+	 *
+	 *	blast_dcache32():
+	 *	    Generic code that flushes the compelte ways with Index cashe op
+	 *	    Index_Writeback_Inv_D. expected to be slower do to flushing
+	 *	    more ways than necessary and apparently flushing both L1 and
+	 *	    L2 caches.
+	 *
+	 *	Test results with WebView APK using r4k_flush_icache_range():
+	 *		blast_dcache32()  AND blast_icache32():    2.5s   [BETTER]
+	 *		blast_dcache_jz() AND blast_dcache_jz():   6.0s
+	 */
+#if 0
+	r4k_blast_dcache_jz = blast_dcache_jz;
+#else
 	r4k_blast_dcache_jz = blast_dcache32;
+#endif
 	if (dc_lsize == 0)
 		r4k_blast_dcache = (void *)cache_noop;
 	else if (dc_lsize == 16)
@@ -284,8 +308,16 @@ static void (* r4k_blast_icache)(void);
 static void __cpuinit r4k_blast_icache_setup(void)
 {
 	unsigned long ic_lsize = cpu_icache_line_size();
-//	r4k_blast_icache_jz = blast_icache_jz;
+
+	/*
+	 * blast_icache32() likely faster than blast_icache_jz().
+	 * See note above in r4k_blast_dcache_setup().
+	 */
+#if 0
+	r4k_blast_icache_jz = blast_icache_jz;
+#else
 	r4k_blast_icache_jz = blast_icache32;
+#endif
 	if (ic_lsize == 0)
 		r4k_blast_icache = (void *)cache_noop;
 	else if (ic_lsize == 16)
@@ -581,6 +613,7 @@ static void r4k_flush_data_cache_page(unsigned long addr)
 	local_r4k_flush_data_cache_page((void *)addr);
 /*
 	if (in_atomic()){
+		// HIGHMEM
 		local_r4k_flush_data_cache_page((void *)addr);
 	}
 	else {
@@ -607,6 +640,7 @@ struct flush_icache_range_args {
 	unsigned long start;
 	unsigned long end;
 };
+
 #if 0
 static inline void local_r4k_flush_icache_range(unsigned long start, unsigned long end)
 {
@@ -644,14 +678,19 @@ static void r4k_flush_icache_range(unsigned long start, unsigned long end)
 	r4k_on_each_cpu(local_r4k_flush_icache_range_ipi, &args);
 	instruction_hazard();
 }
+
 #else
+
 static inline void local_r4k_flush_icache_range(unsigned long start, unsigned long end)
 {
+	preempt_disable();
+
 	if (!cpu_has_ic_fills_f_dc) {
 		if (end - start >= dcache_size) {
                         r4k_blast_dcache();
 		} else {
 			R4600_HIT_CACHEOP_WAR_IMPL;
+			BUG_ON(preemptible());
 			protected_blast_dcache_range(start, end);
 		}
 	}
@@ -660,141 +699,267 @@ static inline void local_r4k_flush_icache_range(unsigned long start, unsigned lo
 		r4k_blast_icache();
 	else
 		protected_blast_icache_range(start, end);
+
+	preempt_enable();
 }
+
 static inline void local_r4k_flush_icache_jz_ipi(void *args){
 	r4k_blast_icache_jz();
 }
+
 static inline void local_r4k_flush_dcache_jz_ipi(void *args){
 	r4k_blast_dcache_jz();
 }
 
 
 /*
-   The other CPU maybe running in different process(different ASID) whit different address mapping,
-   so flush dcache all with local_r4k_flush_dcache_ipi() is safe.
-   But local_r4k_flush_dcache_ipi() flush icache all by index,
-   both flushed L1 icache and L2 cache, it slowdown the machine performance.
-   so optimized the routine as following.
+ * Logging function for the next two cache flushing functions:
+ */
+#define LOG_INFO() {                                                                     \
+    printk("%s(args:%p->{start:0x%lx, end:0x%lx}): ...\n", __func__, args, start, end);  \
+                                                                                         \
+    printk("   ... cpu:%d, start:%#lx, (end-start):%#lx\n",                              \
+                   cpu,    start,      (end-start));                                     \
+                                                                                         \
+    printk("   ... effective_addr_mask:0x%lx, cache_lines:%d\n\n",                       \
+                   effective_addr_mask,       cache_lines);                              \
+}
+
+/* Waiting for confirmation from Jun Jiang. */
+#define DCACHE_ADDRESS_CACHE_OPS_BROADCAST_TO_COHERENT_CACHES 1
+
+#if !DCACHE_ADDRESS_CACHE_OPS_BROADCAST_TO_COHERENT_CACHES
+/*
+ * The other CPU may be running a different process (different ASID) with different address
+ * mappings, however flushing the local dcache all with local_r4k_flush_dcache_ipi() is safe
+ * as the addresses are mapped on the local CPU.
+ *
+ * But local_r4k_flush_dcache_ipi() flushes dcache with all indices, flushing both L1 dcache
+ * and L2 cache this decreases the machine performance (2x ... 3x). So here we optimised the
+ * procedure by flushing to memory each of the ways of each cache line that could match
+ * addresses being flushed.  Only the way and index bits are significant.
+ *
+ * This functions is only needed if the dcache_hit_writeback_invalidate cache op doesn't
+ * broadcast to other coherant caches.
  */
 static inline void protected_blast_other_cpu_dcache_range_ipi(void *args)
 {
-	unsigned long lsize;
+	struct flush_icache_range_args *addrp = (struct flush_icache_range_args *)args;
+	unsigned long dc_mask = dcache_size - 1;
+	unsigned int cpu = smp_processor_id();
+	unsigned long effective_addr_mask;
+	unsigned long start = addrp->start;
+	unsigned long end = addrp->end;
+	static int log_info = 0;
+	unsigned long dc_lsize;
+	unsigned long dc_lmask;
+	int sanity_count = 0;
 	unsigned long addr;
-	unsigned long aend;
-	struct flush_icache_range_args * addrp;
-	unsigned long start;
-	unsigned long end;
+	int cache_lines;
 
-	addrp = (struct flush_icache_range_args *)args;
-	start = addrp->start;
-	end = addrp->end;
-
-	lsize = cpu_dcache_line_size();
-	addr = (start & ~(lsize - 1)) | INDEX_BASE; /* INDEX_BASE = 0x80000000 */
-	aend = ((end - 1) & ~(lsize - 1)) | INDEX_BASE;
-
-	//printk("protected_blast_other_cpu_dcache_range_ipi id=%d, start=%#x, size=%#x\n", smp_processor_id(),  (unsigned int)start, (unsigned int)(end-start));
-
-	while (1) {
-		protected_cache_op(Index_Writeback_Inv_D, (addr + 0x0));
-		protected_cache_op(Index_Writeback_Inv_D, (addr + 0x1000));
-		protected_cache_op(Index_Writeback_Inv_D, (addr + 0x2000));
-		protected_cache_op(Index_Writeback_Inv_D, (addr + 0x3000));
-		protected_cache_op(Index_Writeback_Inv_D, (addr + 0x4000));
-		protected_cache_op(Index_Writeback_Inv_D, (addr + 0x5000));
-		protected_cache_op(Index_Writeback_Inv_D, (addr + 0x6000));
-		protected_cache_op(Index_Writeback_Inv_D, (addr + 0x7000));
-		if (addr == aend)
-			break;
-		addr += lsize;
+	if ( preemptible() )
+		panic("%s: preemptible!\n", __func__);
+
+	dc_lsize = cpu_dcache_line_size();
+	dc_lmask = ~(dc_lsize - 1);
+	cache_lines = (dcache_size / dc_lsize);
+        effective_addr_mask = dc_mask & dc_lmask;
+
+	if (log_info > 0) {
+		LOG_INFO();
+		log_info--;
 	}
+	for (addr = start; addr <= end; addr += dc_lsize) {
+		unsigned long effective_addr;
+
+		/* Using KSEG0 (INDEX_BASE:0x8000,0000) which is always mapped */
+		effective_addr = (addr & effective_addr_mask) | INDEX_BASE;
 
+		/*
+		 * Flush each way for the current index to memory.
+		 *
+		 * NOTE: For the Ingenic 4780 this instruction flushes both the
+		 *       the 1st level data and the general 2nd level cache lines.
+		 *	 For MIPS this instruction typically only flushes the
+		 *       1st level data cache.
+		 */
+		protected_cache_op(Index_Writeback_Inv_D, (effective_addr + 0x0));
+		protected_cache_op(Index_Writeback_Inv_D, (effective_addr + 0x1000));
+		protected_cache_op(Index_Writeback_Inv_D, (effective_addr + 0x2000));
+		protected_cache_op(Index_Writeback_Inv_D, (effective_addr + 0x3000));
+		protected_cache_op(Index_Writeback_Inv_D, (effective_addr + 0x4000));
+		protected_cache_op(Index_Writeback_Inv_D, (effective_addr + 0x5000));
+		protected_cache_op(Index_Writeback_Inv_D, (effective_addr + 0x6000));
+		protected_cache_op(Index_Writeback_Inv_D, (effective_addr + 0x7000));
+
+		if (sanity_count++ > cache_lines) {
+			LOG_INFO();
+			panic(__func__);
+			/* NOTREACHED */
+		}
+	}
 	SYNC_WB();
 }
+#endif
 
 /*
-   The other CPU maybe running in different process(different ASID) whit different address mapping,
-   so flush icache all with local_r4k_flush_icache_ipi() is safe.
-   But local_r4k_flush_icache_ipi() flush icache all by index,
-   both flushed L1 icache and L2 cache, it slowdown the machine performance.
-   so optimized the routine as following.
+ * The other CPU may be running a different process (different ASID) with different address
+ * mappings, however flushing the local icache all with local_r4k_flush_icache_ipi() is safe
+ * as the addresses are mapped on the local CPU.
+ *
+ * But local_r4k_flush_icache_ipi() flushes icache with all indices, flushing both L1 icache
+ * and L2 cache this decreases the machine performance (2x ... 3x). So here we optimised the
+ * procedure by flushing to memory each of the ways of each cache line that could match
+ * addresses being flushed.  Only the way and index bits are significant.
+ *
+ * I-Cache isn't coherant so local address cache ops are not broadcast. From Ingenic:
+ * 	"For JZ4780, coherence implementation is only available among local cores' dcache.
+ *	 While coherence problem among different cores' icache has to be solved by software.
+ *	 Thus, for SYNCI, it flushes only the icache on the local core."
  */
 static inline void protected_blast_other_cpu_icache_range_ipi(void *args)
 {
-	unsigned long lsize;
+	struct flush_icache_range_args *addrp = (struct flush_icache_range_args *)args;
+	unsigned long ic_mask = icache_size - 1;
+	unsigned int cpu = smp_processor_id();
+	unsigned long effective_addr_mask;
+	unsigned long start = addrp->start;
+	unsigned long end = addrp->end;
+	static int log_info = 0;
+	unsigned long ic_lsize;
+	unsigned long ic_lmask;
+	int sanity_count = 0;
 	unsigned long addr;
-	unsigned long aend;
-	struct flush_icache_range_args * addrp;
-	unsigned long start;
-	unsigned long end;
+	int cache_lines;
 
-	addrp = (struct flush_icache_range_args *)args;
-	start = addrp->start;
-	end = addrp->end;
-
-	lsize = cpu_icache_line_size();
-	addr = (start & ~(lsize - 1)) | INDEX_BASE; /* INDEX_BASE = 0x80000000 */
-	aend = ((end - 1) & ~(lsize - 1)) | INDEX_BASE;
-
-	//printk("protected_blast_other_cpu_icache_range_ipi id=%d, start=%#x, size=%#x\n", smp_processor_id(),  (unsigned int)start, (unsigned int)(end-start));
-
-	while (1) {
-		protected_cache_op(Index_Invalidate_I, (addr + 0x0));
-		protected_cache_op(Index_Invalidate_I, (addr + 0x1000));
-		protected_cache_op(Index_Invalidate_I, (addr + 0x2000));
-		protected_cache_op(Index_Invalidate_I, (addr + 0x3000));
-		protected_cache_op(Index_Invalidate_I, (addr + 0x4000));
-		protected_cache_op(Index_Invalidate_I, (addr + 0x5000));
-		protected_cache_op(Index_Invalidate_I, (addr + 0x6000));
-		protected_cache_op(Index_Invalidate_I, (addr + 0x7000));
-		if (addr == aend)
-			break;
-		addr += lsize;
+	if ( preemptible() )
+		printk("%s: preemptible!\n", __func__);
+
+	ic_lsize = cpu_icache_line_size();
+	ic_lmask = ~(ic_lsize - 1);
+	cache_lines = (icache_size / ic_lsize);
+	effective_addr_mask = ic_mask & ic_lmask;
+
+	if (log_info > 0) {
+		LOG_INFO();
+		log_info--;
+	}
+	for (addr = start; addr <= end; addr += ic_lsize) {
+		unsigned long effective_addr;
+
+		/* Using KSEG0 (INDEX_BASE:0x8000,0000) which is always mapped */
+		effective_addr = (addr & effective_addr_mask) | INDEX_BASE;
+
+		/*
+		 * Flush each way for the current index to memory.
+		 */
+		protected_cache_op(Index_Invalidate_I, (effective_addr + 0x0));
+		protected_cache_op(Index_Invalidate_I, (effective_addr + 0x1000));
+		protected_cache_op(Index_Invalidate_I, (effective_addr + 0x2000));
+		protected_cache_op(Index_Invalidate_I, (effective_addr + 0x3000));
+		protected_cache_op(Index_Invalidate_I, (effective_addr + 0x4000));
+		protected_cache_op(Index_Invalidate_I, (effective_addr + 0x5000));
+		protected_cache_op(Index_Invalidate_I, (effective_addr + 0x6000));
+		protected_cache_op(Index_Invalidate_I, (effective_addr + 0x7000));
+
+		if (sanity_count++ > cache_lines) {
+			LOG_INFO();
+			panic(__func__);
+			/* NOTREACHED */
+		}
 	}
 	INVALIDATE_BTB();
 }
 
 /*
- * Test result by 2013-10-25:
- * Flush complete dcache and icache on other CPUs with local_r4k_flush_dcache_jz_ipi()
- * and local_r4k_flush_icache_jz_ipi(), running flushtest is stable 4days.
+ * Test results on 22-Nov-2013:
+ * ----------------------------
+ * Selective flushing of a short range of addresses via:
  *
- * Flush dcache_range and icache_range by index on other CPUs,
- * with protected_blast_other_cpu_dcache_range_ipi() protected_blast_other_cpu_icache_range_ipi(),
- * fails running flushtest after hours. It should be update in the future.
+ *	protected_blast_other_cpu_icache_range_ipi()
+ * and
+ *	protected_blast_other_cpu_dcache_range_ipi()
+ *
+ * is now stable with the addition of disabling pre-emption and is
+ * 2x to 3x faster with browser than flushing the complete dcache
+ * and icache on other CPUs with:
+ *
+ *	local_r4k_flush_dcache_jz_ipi()
+ * and
+ *	local_r4k_flush_icache_jz_ipi().
+ *
+ * flushtest is rock solid.
+ *
+ * Processor must not be preemptible while doing cache flush operations;
+ * and needs to be disable here as we all called direclty from a system call:
+ *
+ * sys_cacheflush() {
+ *    flush_icache_range() --> r4k_flush_icache_range() {
+ *        r4k_flush_icache_range(); 				YOUR ARE HERE
+ *    }
+ * }
  */
 static void r4k_flush_icache_range(unsigned long start, unsigned long end)
 {
+	preempt_disable();
+
 	if (!cpu_has_ic_fills_f_dc) {
 		if (end - start >= dcache_size) {
                         /* Flush complete dcache on all CPUs */
-			r4k_on_each_cpu(local_r4k_flush_dcache_jz_ipi,0);
+			smp_call_function(local_r4k_flush_dcache_jz_ipi, 0, 1);
 		} else {
-                        /* Flush dcache by address on this CPU */
+                        /*
+			 * Flush dcache by address on this CPU;
+			 * likely broadcast to other cores.
+			 */
+			BUG_ON(preemptible());
 			protected_blast_dcache_range(start, end);
+
+#if !DCACHE_ADDRESS_CACHE_OPS_BROADCAST_TO_COHERENT_CACHES
+			/*
+			 * This can likely be dropped/disabled; as implemented by Stephen Qiu.
+			 */
+			if ( (end-start < current_cpu_data.dcache.waysize)) {
+				/* Flush dcache_range by index on other CPUs */
+				struct flush_icache_range_args range_addr;
+				range_addr.start = start;
+				range_addr.end = end;
+				smp_call_function(protected_blast_other_cpu_dcache_range_ipi,
+							&range_addr, 1);
+			}
+			else {
+				/* Flush complete dcache on other CPUs */
+				smp_call_function(local_r4k_flush_dcache_jz_ipi, 0, 1);
+			}
+#endif
 		}
 	}
 
 	if (end - start >= icache_size)
                 /* Flush complete icache on all CPUs */
-		r4k_on_each_cpu(local_r4k_flush_icache_jz_ipi,0);
+		smp_call_function(local_r4k_flush_icache_jz_ipi, 0, 1);
 	else {
-		preempt_disable();
-                /* Flush icache by address on this CPU */
+                /*
+		 * Flush icache by address on this CPU;
+		 * NOT broadcast to other cores.
+		 */
+		BUG_ON(preemptible());
 		protected_blast_icache_range(start, end);
-		if (end-start <= PAGE_SIZE) {
+
+		if ( (end-start < current_cpu_data.icache.waysize)) {
 			/* Flush icache_range by index on other CPUs */
 			struct flush_icache_range_args range_addr;
 			range_addr.start = start;
 			range_addr.end = end;
-			smp_call_function(protected_blast_other_cpu_icache_range_ipi, &range_addr,1);
+			smp_call_function(protected_blast_other_cpu_icache_range_ipi,
+						&range_addr, 1);
 		}
 		else {
 			/* Flush complete icache on other CPUs */
-			smp_call_function(local_r4k_flush_icache_jz_ipi,0,1);
+			smp_call_function(local_r4k_flush_icache_jz_ipi, 0, 1);
 		}
-		preempt_enable();
         }
+	preempt_enable();
 }
 #endif
 
-- 
1.8.0.rc0

